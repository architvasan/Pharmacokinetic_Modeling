embeddings MolformerEmbeddings(
  (word_embeddings): Embedding(2362, 768, padding_idx=2)
  (dropout): Dropout(p=0.2, inplace=False)
)
encoder MolformerEncoder(
  (layer): ModuleList(
    (0-11): 12 x MolformerLayer(
      (attention): MolformerAttention(
        (self): MolformerSelfAttention(
          (query): Linear(in_features=768, out_features=768, bias=True)
          (key): Linear(in_features=768, out_features=768, bias=True)
          (value): Linear(in_features=768, out_features=768, bias=True)
          (rotary_embeddings): MolformerRotaryEmbedding()
          (feature_map): MolformerFeatureMap(
            (kernel): ReLU()
          )
        )
        (output): MolformerSelfOutput(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (intermediate): MolformerIntermediate(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (intermediate_act_fn): GELUActivation()
      )
      (output): MolformerOutput(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
)
LayerNorm LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                              SMILES   label
0                                   Brc1cccc2ccccc12 -4.3500
1                             N#Cc1cc(Br)c(O)c(Br)c1 -3.3300
2                               Brc1cc(Br)c(Br)cc1Br -6.9800
3                     COP(=S)(OC)Oc1cc(Cl)c(Br)cc1Cl -6.0900
4                       CON(C)C(=O)Nc1ccc(Br)c(Cl)c1 -3.9200
...                                              ...     ...
2955   NS(=O)(=O)c1ccc(C(=O)c2ccc(CNCc3ccccc3)cc2)s1 -3.3319
2956        CCCCNCc1ccc(C(=O)c2ccc(S(N)(=O)=O)s2)cc1 -2.1669
2957     NS(=O)(=O)c1ccc(C(=O)c2ccc(CN3CCOCC3)cc2)s1 -1.4812
2958  CN1CCN(Cc2ccc(C(=O)c3ccc(S(N)(=O)=O)s3)cc2)CC1 -1.8802
2959     CCN(CC)Cc1cc(C(=O)c2ccc(S(N)(=O)=O)s2)ccc1O -1.0324
[2960 rows x 2 columns]
Epoch 0
num_of_examples 1 loss: 0.04550333023071289 %_data_trained : 0.0
num_of_examples 81 loss: 0.23479702770709993 %_data_trained : 0.3382663847780127
num_of_examples 161 loss: 0.22734447717666625 %_data_trained : 0.6765327695560254
/lus/eagle/projects/datascience/avasan/PharmacoKinetics/ModelTraining/MolFormer/Solubility/data_utils.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  "y_regression_values": torch.tensor(self.y_regression_values[idx]).to(self.device),
num_of_examples 241 loss: 0.23510238528251648 %_data_trained : 1.014799154334038
num_of_examples 321 loss: 0.205815652012825 %_data_trained : 1.3530655391120507
num_of_examples 401 loss: 0.1935615658760071 %_data_trained : 1.6913319238900635
num_of_examples 481 loss: 0.18167940378189087 %_data_trained : 2.029598308668076
num_of_examples 561 loss: 0.19634297788143157 %_data_trained : 2.3678646934460885
num_of_examples 641 loss: 0.17440711855888366 %_data_trained : 2.7061310782241015
num_of_examples 721 loss: 0.1654548615217209 %_data_trained : 3.044397463002114
num_of_examples 801 loss: 0.16957612037658693 %_data_trained : 3.382663847780127
num_of_examples 881 loss: 0.14450901746749878 %_data_trained : 3.7209302325581395
num_of_examples 961 loss: 0.14829520881175995 %_data_trained : 4.059196617336152
num_of_examples 1041 loss: 0.13717717826366424 %_data_trained : 4.397463002114165
num_of_examples 1121 loss: 0.1216091737151146 %_data_trained : 4.735729386892177
num_of_examples 1201 loss: 0.11452552229166031 %_data_trained : 5.07399577167019
num_of_examples 1281 loss: 0.08619946837425232 %_data_trained : 5.412262156448203
num_of_examples 1361 loss: 0.07562782615423203 %_data_trained : 5.750528541226215
num_of_examples 1441 loss: 0.07565086781978607 %_data_trained : 6.088794926004228
num_of_examples 1521 loss: 0.05865666717290878 %_data_trained : 6.427061310782241
num_of_examples 1601 loss: 0.052983860671520236 %_data_trained : 6.765327695560254
num_of_examples 1681 loss: 0.04079494625329971 %_data_trained : 7.103594080338267
num_of_examples 1761 loss: 0.024086520448327063 %_data_trained : 7.441860465116279
num_of_examples 1841 loss: 0.027115924656391142 %_data_trained : 7.780126849894292
num_of_examples 1921 loss: 0.02205701805651188 %_data_trained : 8.118393234672304
num_of_examples 2001 loss: 0.013605760969221592 %_data_trained : 8.456659619450317
num_of_examples 2081 loss: 0.012654922157526016 %_data_trained : 8.79492600422833
num_of_examples 2161 loss: 0.012837192229926585 %_data_trained : 9.133192389006343
num_of_examples 2241 loss: 0.011587756872177123 %_data_trained : 9.471458773784354
num_of_examples 2321 loss: 0.008758351672440767 %_data_trained : 9.809725158562369
  num_of_examples 1 test_loss: 7.848570123314858e-05
Test r2: 0.013271724088637082
Traceback (most recent call last):
  File "/lus/eagle/projects/datascience/avasan/PharmacoKinetics/ModelTraining/MolFormer/Solubility/run_script.py", line 216, in <module>
    model_path = 'model_{}_{}'.format(timestamp, num_of_examples)
                                                 ^^^^^^^^^^^^^^^
NameError: name 'num_of_examples' is not defined