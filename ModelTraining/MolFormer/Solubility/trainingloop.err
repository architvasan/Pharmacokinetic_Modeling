wandb: Currently logged in as: avasan (anl-archit). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /lus/eagle/projects/datascience/avasan/PharmacoKinetics/ModelTraining/MolFormer/Solubility/wandb/run-20240520_010333-2ruiy5m4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-deluge-27
wandb: ⭐️ View project at https://wandb.ai/anl-archit/uncategorized
wandb: 🚀 View run at https://wandb.ai/anl-archit/uncategorized/runs/2ruiy5m4
/lus/eagle/projects/datascience/avasan/PharmacoKinetics/ModelTraining/MolFormer/Solubility/data_utils.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  "y_regression_values": torch.tensor(self.y_regression_values[idx]).to(self.device),
/lus/eagle/projects/datascience/avasan/PharmacoKinetics/ModelTraining/MolFormer/Solubility/data_utils.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  "y_regression_values": torch.tensor(self.y_regression_values[idx]).to(self.device),
/lus/eagle/projects/datascience/avasan/PharmacoKinetics/ModelTraining/MolFormer/Solubility/data_utils.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  "y_regression_values": torch.tensor(self.y_regression_values[idx]).to(self.device),
/lus/eagle/projects/datascience/avasan/PharmacoKinetics/ModelTraining/MolFormer/Solubility/data_utils.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  "y_regression_values": torch.tensor(self.y_regression_values[idx]).to(self.device),
/lus/eagle/projects/datascience/avasan/PharmacoKinetics/ModelTraining/MolFormer/Solubility/data_utils.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  "y_regression_values": torch.tensor(self.y_regression_values[idx]).to(self.device),
/lus/eagle/projects/datascience/avasan/PharmacoKinetics/ModelTraining/MolFormer/Solubility/data_utils.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  "y_regression_values": torch.tensor(self.y_regression_values[idx]).to(self.device),
/lus/eagle/projects/datascience/avasan/PharmacoKinetics/ModelTraining/MolFormer/Solubility/data_utils.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  "y_regression_values": torch.tensor(self.y_regression_values[idx]).to(self.device),
Traceback (most recent call last):
  File "/lus/eagle/projects/datascience/avasan/PharmacoKinetics/ModelTraining/MolFormer/Solubility/run_script.py", line 208, in <module>
    train_one_epoch(epoch)
  File "/lus/eagle/projects/datascience/avasan/PharmacoKinetics/ModelTraining/MolFormer/Solubility/run_script.py", line 128, in train_one_epoch
    total_loss += nn_loss.item()
                  ^^^^^^^^^^^^^^
KeyboardInterrupt
Exception ignored in atexit callback: <function _Manager._atexit_setup.<locals>.<lambda> at 0x15308a847420>
Traceback (most recent call last):
  File "/lus/eagle/projects/datascience/avasan/envs/chem_llm_finetune/lib/python3.11/site-packages/wandb/sdk/wandb_manager.py", line 156, in <lambda>
    self._atexit_lambda = lambda: self._atexit_teardown()
                                  ^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/avasan/envs/chem_llm_finetune/lib/python3.11/site-packages/wandb/sdk/wandb_manager.py", line 165, in _atexit_teardown
    self._teardown(exit_code)
  File "/lus/eagle/projects/datascience/avasan/envs/chem_llm_finetune/lib/python3.11/site-packages/wandb/sdk/wandb_manager.py", line 176, in _teardown
    result = self._service.join()
             ^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/avasan/envs/chem_llm_finetune/lib/python3.11/site-packages/wandb/sdk/service/service.py", line 263, in join
    ret = self._internal_proc.wait()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lus/eagle/projects/datascience/avasan/envs/chem_llm_finetune/lib/python3.11/subprocess.py", line 1277, in wait
    self._wait(timeout=sigint_timeout)
  File "/lus/eagle/projects/datascience/avasan/envs/chem_llm_finetune/lib/python3.11/subprocess.py", line 2045, in _wait
    time.sleep(delay)
KeyboardInterrupt: 
wandb: - 0.008 MB of 0.008 MB uploadedwandb: \ 0.079 MB of 0.079 MB uploadedwandb: 
wandb: Run history:
wandb:      num_of_examples ▁▂▄▅▆▇▁▂▄▅▆▇▁▃▄▅▆█▁▃▄▅▆█▁▃▄▅▇█▂▃▄▅▇█▂▃▄▆
wandb: num_of_test_examples ▁▁▁▁▁▁
wandb:            test_loss █▂▁▁▁▁
wandb:           train_loss █▇▆▆▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:      num_of_examples 1600
wandb: num_of_test_examples 0
wandb:            test_loss 4e-05
wandb:           train_loss 0.00267
wandb: 
wandb: 🚀 View run cool-deluge-27 at: https://wandb.ai/anl-archit/uncategorized/runs/2ruiy5m4
wandb: ⭐️ View project at: https://wandb.ai/anl-archit/uncategorized
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240520_010333-2ruiy5m4/logs
